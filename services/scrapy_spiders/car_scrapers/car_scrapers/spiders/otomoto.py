#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from rich.console import Console
from rich.progress import Progress, TaskID, BarColumn, TextColumn, TimeRemainingColumn
from rich.live import Live
from rich.table import Table

import json
import math
import time
import scrapy
import urllib.parse as up
from collections import OrderedDict
from datetime import datetime, timezone
from ..items import ParsedAdItem, ActiveIdsItem
from ..utils.make_loader import MakeLoader
from typing import Optional, AsyncGenerator
from scrapy import Request


class OtomotoSpider(scrapy.Spider):
    name = "otomoto"
    allowed_domains = ["otomoto.pl"]

    BASE_URL = "https://www.otomoto.pl/graphql"
    OPERATION_NAME = "listingScreen"
    ITEMS_PER_PAGE = 50

    EXTENSIONS = OrderedDict([
        ("persistedQuery", OrderedDict([
            ("sha256Hash", "1a840f0ab7fbe2543d0d6921f6c963de8341e04a4548fd1733b4a771392f900a"),
            ("version", 1),
        ]))
    ])

    BASE_FILTERS = [
        {"name": "category_id", "value": "29"},
        {"name": "new_used", "value": "used"},
    ]

    BASE_PARAMS = [
        "make", "vin", "offer_type", "show_pir", "fuel_type", "gearbox",
        "country_origin", "mileage", "engine_capacity", "color", "engine_code", 
        "transmission", "engine_power", "first_registration_year",
        "model", "version", "year", "generation"
    ]

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        """–°–æ–∑–¥–∞–µ—Ç spider —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º crawler'–∞"""
        spider = cls(*args, **kwargs)
        spider._set_crawler(crawler)
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ settings –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è spider
        spider.max_consecutive_403 = crawler.settings.getint('CONSECUTIVE_403_LIMIT', 3)
        spider.pause_duration = crawler.settings.getint('PAUSE_DURATION', 300)
        spider.graphql_retry_delay = crawler.settings.getint('GRAPHQL_RETRY_DELAY', 5)
        spider.graphql_max_retries = crawler.settings.getint('GRAPHQL_MAX_RETRIES', 3)
        
        spider.logger.info(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏ 403: –ª–∏–º–∏—Ç={spider.max_consecutive_403}, –ø–∞—É–∑–∞={spider.pause_duration}—Å")
        spider.logger.info(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏ GraphQL: –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {spider.graphql_retry_delay}—Å, –º–∞–∫—Å –ø–æ–≤—Ç–æ—Ä–æ–≤={spider.graphql_max_retries}")
        
        return spider

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.scraped_ids = set()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–∞—Ä–æ–∫
        make_loader = MakeLoader(self.logger)
        self.makes_list = make_loader.get_makes()
        self.current_make_index = 0
        self.current_make_name = None
        self.current_make_active_ids = set()

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è Rich
        self.stats_start_time = time.time()
        self.last_stats_update = time.time()
        self.stats_update_interval = 5
        
        # Rich Progress Bar
        self.console = Console(width=120)
        self.progress = Progress(
            TextColumn("[bold blue]{task.description}"),
            BarColumn(bar_width=25),  # –£–º–µ–Ω—å—à–∞–µ–º —à–∏—Ä–∏–Ω—É –±–∞—Ä–∞
            TextColumn("[progress.percentage]{task.percentage:>3.1f}%"),
            TextColumn("‚Ä¢"),
            TextColumn("{task.completed}/{task.total}"),
            TextColumn("‚Ä¢"),
            TimeRemainingColumn(),
            console=self.console,
            refresh_per_second=2,
            expand=True
        )
        self.main_task: Optional[TaskID] = None
        self.stats_task_1: Optional[TaskID] = None
        self.stats_task_2: Optional[TaskID] = None
        self.current_make_task: Optional[TaskID] = None
        self.progress_live: Optional[Live] = None

        self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.makes_list)} –º–∞—Ä–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
        
        # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.current_make_total_pages = 0
        self.current_make_processed_pages = 0
        self.make_completion_lock = False
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
        self.error_stats = {
            'forbidden_403': 0,
            'json_decode_errors': 0,
            'graphql_errors': 0,
            'graphql_retries': 0,
            'missing_data_errors': 0
        }
        
        # –°—á–µ—Ç—á–∏–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö 403 –æ—à–∏–±–æ–∫ (–∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        self.consecutive_403_count = 0
        self.max_consecutive_403 = 3  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.pause_duration = 300  # 5 –º–∏–Ω—É—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.is_paused = False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è GraphQL –æ—à–∏–±–æ–∫ (–∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        self.graphql_retry_delay = 5  # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
        self.graphql_max_retries = 3  # –ú–∞–∫—Å–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–æ–≤
        
        self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.makes_list)} –º–∞—Ä–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")


    async def start(self) -> AsyncGenerator[Request, None]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
        if not self.makes_list:
            self.logger.error("–ù–µ—Ç –º–∞—Ä–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∑–∞–≥—Ä—É–∑–∫—É —Å–ø–∏—Å–∫–∞ –º–∞—Ä–æ–∫.")
            return
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        self._start_progress_bar()

        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ø–µ—Ä–≤–æ–π –º–∞—Ä–∫–∏
        request = self._get_request_for_current_make()
        if request:
            yield request


    def _start_progress_bar(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç Rich Progress Bar –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        self.console = Console(
            width=120,
            legacy_windows=False
            )
        self.progress_live = Live(
            self.progress,
            console=self.console,
            refresh_per_second=2,
            auto_refresh=True,
            transient=False
            )        
        self.progress_live.start()

        # –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ - –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ –º–∞—Ä–∫–∞–º
        self.main_task = self.progress.add_task(
            "[green]üìà –û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞",
            total=len(self.makes_list),
        )
        
        # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats_task_1 = self.progress.add_task(
            "[cyan]üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...",
            total=100,
            visible=True
        )
        
        # –í—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats_task_2 = self.progress.add_task(
            "[cyan]‚è±Ô∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∞–π–º–µ—Ä–∞...",
            total=100,
            visible=True
        )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self._update_scrapy_stats()

    
    def _update_scrapy_stats(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É Scrapy –≤ Rich –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–µ"""
        if hasattr(self, 'crawler') and hasattr(self, 'stats_task_1') and hasattr(self, 'stats_task_2'):
            stats = self.crawler.stats
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            pages_crawled = stats.get_value('response_received_count', 0)
            items_scraped = stats.get_value('item_scraped_count', 0)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å
            current_time = time.time()
            elapsed_time = current_time - self.stats_start_time
            
            if elapsed_time > 0:
                pages_per_min = (pages_crawled / elapsed_time) * 60
                items_per_min = (items_scraped / elapsed_time) * 60
            else:
                pages_per_min = 0
                items_per_min = 0
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
            elapsed_str = time.strftime('%H:%M:%S', time.gmtime(elapsed_time))
            
            # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ - –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö
            stats_description_1 = (
                f"[cyan]üìä[/cyan] –°—Ç—Ä–∞–Ω–∏—Ü: [bold]{pages_crawled}[/bold] ‚Ä¢ "
                f"–û–±—ä—è–≤–ª–µ–Ω–∏–π: [bold]{items_scraped}[/bold] ‚Ä¢ "
                f"–û—à–∏–±–∫–∏: [red]{self.error_stats['forbidden_403']}[/red] (403), "
                f"[red]{self.error_stats['graphql_errors']}[/red] (GraphQL)"
            )
            
            # –í—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ - —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –≤—Ä–µ–º—è
            stats_description_2 = (
                f"[cyan]‚è±Ô∏è[/cyan] –°–∫–æ—Ä–æ—Å—Ç—å: [bold]{pages_per_min:.0f}[/bold] —Å—Ç—Ä/–º–∏–Ω, "
                f"[bold]{items_per_min:.0f}[/bold] –æ–±—ä—è–≤–ª/–º–∏–Ω ‚Ä¢ "
                f"–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: [bold]{elapsed_str}[/bold]"
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±–µ —Å—Ç—Ä–æ–∫–∏
            self.progress.update(
                self.stats_task_1,
                completed=50,
                description=stats_description_1
            )
            
            self.progress.update(
                self.stats_task_2,
                completed=50,
                description=stats_description_2
            )
        

    def _get_request_for_current_make(self):
        """–°–æ–∑–¥–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—É—â–µ–π –º–∞—Ä–∫–∏"""
        if self.current_make_index >= len(self.makes_list):
            self.logger.info("–í—Å–µ –º–∞—Ä–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            self._stop_progress_bar()
            return None
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –º–∞—Ä–∫—É
        self.current_make_name = self.makes_list[self.current_make_index]
        self.current_make_active_ids = set()
        self.current_make_total_pages = 0
        self.current_make_processed_pages = 0
        self.make_completion_lock = False
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å
        if self.main_task is not None:
            self.progress.update(
                self.main_task,
                completed=self.current_make_index,
                description=f"[green]–ü–∞—Ä—Å–∏–Ω–≥ –º–∞—Ä–∫–∏: [bold cyan]{self.current_make_name}[/bold cyan]",
            )

        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–∞—Ä–∫–∏
        if self.current_make_task is not None:
            self.progress.remove_task(self.current_make_task)

        self.current_make_task = self.progress.add_task(
            f"[yellow]{self.current_make_name}[/yellow] - –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...",
            total = None
        )

        self.logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –º–∞—Ä–∫–∏: {self.current_make_name} ({self.current_make_index + 1}/{len(self.makes_list)})")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–∞—Ä–∫–∏
        self._update_filters_for_make(self.current_make_name)
        
        return scrapy.Request(
            url=self.build_url(page=1),
            callback=self.parse_initial,
            meta={'handle_httpstatus_list': [403], 'make_name': self.current_make_name}
        )


    def _update_filters_for_make(self, make_name):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–∞—Ä–∫–∏"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–∏–ª—å—Ç—Ä –º–∞—Ä–∫–∏
        self.BASE_FILTERS = [f for f in self.BASE_FILTERS if f.get('name') != 'filter_enum_make']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ñ–∏–ª—å—Ç—Ä –º–∞—Ä–∫–∏
        self.BASE_FILTERS.append({"name": "filter_enum_make", "value": make_name})


    def build_url(self, page: int) -> str:
        # ...existing code... (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å)
        variables = OrderedDict([
            ("filters", self.BASE_FILTERS),
            ("includeCepik", False),
            ("includeFiltersCounters", True),
            ("includeNewPromotedAds", False),
            ("includePriceEvaluation", False),
            ("includePromotedAds", False),
            ("includeRatings", False),
            ("includeSortOptions", False),
            ("includeSuggestedFilters", False),
            ("itemsPerPage", self.ITEMS_PER_PAGE),
            ("maxAge", 60),
            ("page", page),
            ("parameters", self.BASE_PARAMS),
            ("promotedInput", {})
        ])

        vars_compact = json.dumps(variables, separators=(',', ':'))
        ext_compact = json.dumps(self.EXTENSIONS, separators=(',', ':'))

        encoded_vars = up.quote(vars_compact)
        encoded_ext = up.quote(ext_compact)

        url = f"{self.BASE_URL}?operationName={self.OPERATION_NAME}&variables={encoded_vars}&extensions={encoded_ext}"
        return url


    def parse_initial(self, response):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–µ—Ä–≤—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –º–∞—Ä–∫–∏, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞ –ø–∞—É–∑–µ –ª–∏ –º—ã
        if self.is_paused:
            self.logger.info("–ü–∞—Ä—Å–µ—Ä –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –ø–∞—É–∑–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å")
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
        if response.status == 403:
            request = self._handle_403_error(response, context=f"parse_initial –¥–ª—è –º–∞—Ä–∫–∏ {self.current_make_name}")
            if request:
                yield request
                return
            else:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–∫—É—â—É—é –º–∞—Ä–∫—É –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π
                yield from self._handle_make_completion()
                return
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ 403 –æ—à–∏–±–æ–∫
        self._reset_403_counter_on_success()
        
        try:
            data = json.loads(response.text)
        except json.JSONDecodeError:
            self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å JSON —Å {response.url}")
            self.error_stats['json_decode_errors'] += 1
            yield from self._handle_make_completion()
            return

        if 'errors' in data:
            retry_request = self._handle_graphql_error(response, data['errors'], f"parse_initial –¥–ª—è –º–∞—Ä–∫–∏ {self.current_make_name}")
            if retry_request:
                yield retry_request
                return
            else:
                yield from self._handle_make_completion()
                return

        advert_search_data = data.get('data', {}).get('advertSearch')
        if not advert_search_data:
            self.logger.error(f"–ö–ª—é—á 'advertSearch' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            self.error_stats['missing_data_errors'] += 1
            yield from self._handle_make_completion()
            return

        total_ads = advert_search_data.get('totalCount', 0)
        self.current_make_total_pages = math.ceil(total_ads / self.ITEMS_PER_PAGE)

        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–¥–∞—á—É —Ç–µ–∫—É—â–µ–π –º–∞—Ä–∫–∏
        if self.current_make_task is not None:
            if total_ads > 0:
                self.progress.update(
                    self.current_make_task,
                    total=self.current_make_total_pages,
                    completed=0,
                    description=f"[yellow]{self.current_make_name}[/yellow] - {total_ads} –æ–±—ä—è–≤–ª–µ–Ω–∏–π"
                )
            else:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è —Å 0 –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏
                self.progress.update(
                    self.current_make_task,
                    total=1,
                    completed=0,
                    description=f"[yellow]{self.current_make_name}[/yellow] - –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π"
                )
        
        self.logger.info(f"–ú–∞—Ä–∫–∞ {self.current_make_name}: –Ω–∞–π–¥–µ–Ω–æ {total_ads} –æ–±—ä—è–≤–ª–µ–Ω–∏–π, —Å—Ç—Ä–∞–Ω–∏—Ü: {self.current_make_total_pages}")

        # –ï—Å–ª–∏ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π, —Å—Ä–∞–∑—É –∑–∞–≤–µ—Ä—à–∞–µ–º –º–∞—Ä–∫—É
        if total_ads == 0:
            yield from self._handle_make_completion()
            return

        # –ü–∞—Ä—Å–∏–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        yield from self.parse_page(response, response.meta)

        # –ó–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        for page_num in range(2, self.current_make_total_pages + 1):
            yield scrapy.Request(
                url=self.build_url(page=page_num),
                callback=self.parse_page,
                meta={'page_num': page_num, 'handle_httpstatus_list': [403], 'make_name': self.current_make_name}
            )
            

    def parse_page(self, response, meta=None):
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏"""
        current_meta = meta if meta else response.meta
        page_num = current_meta.get('page_num', 1)
        make_name = current_meta.get('make_name', self.current_make_name)

        current_time = time.time()
        if current_time - self.last_stats_update >= self.stats_update_interval:
            self._update_scrapy_stats()
            self.last_stats_update = current_time

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞ –ø–∞—É–∑–µ –ª–∏ –º—ã
        if self.is_paused:
            self.logger.info("–ü–∞—Ä—Å–µ—Ä –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –ø–∞—É–∑–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å")
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
        if response.status == 403:
            pause_request = self._handle_403_error(response, context=f"parse_page –º–∞—Ä–∫–∏ {make_name}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
            if pause_request:
                yield pause_request
                return
            else:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                yield from self._handle_page_completion()
                return
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ 403 –æ—à–∏–±–æ–∫
        self._reset_403_counter_on_success()

        try:
            data = json.loads(response.text)
        except json.JSONDecodeError:
            self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å JSON –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num} —Å {response.url}")
            self.error_stats['json_decode_errors'] += 1
            yield from self._handle_page_completion()
            return

        if 'errors' in data:
            retry_request = self._handle_graphql_error(response, data['errors'], f"parse_page –º–∞—Ä–∫–∏ {make_name}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
            if retry_request:
                yield retry_request
                return
            else:
                yield from self._handle_page_completion()
                return

        advert_search_data = data.get('data', {}).get('advertSearch')
        if not advert_search_data:
            self.logger.error(f"–ö–ª—é—á 'advertSearch' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ JSON –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {response.text[:500]}")
            self.error_stats['missing_data_errors'] += 1
            yield from self._handle_page_completion()
            return

        edges = advert_search_data.get('edges', [])

        # –û–±—ã—á–Ω—ã–π –ª–æ–≥ —Ç–æ–ª—å–∫–æ –¥–ª—è –∑–Ω–∞—á–∏–º—ã—Ö —Å–æ–±—ã—Ç–∏–π
        if page_num % 10 == 1 or page_num == self.current_make_total_pages:  # –ö–∞–∂–¥–∞—è 10-—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è
            self.logger.info(f"–ú–∞—Ä–∫–∞ {make_name}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –Ω–∞–π–¥–µ–Ω–æ {len(edges)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")

        for edge in edges:
            node = edge.get('node', {})
            if not node:
                continue

            # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ---
            raw_params = node.get('parameters', [])
            params = {p.get('key'): p.get('value') for p in raw_params if p.get('key') and p.get('value') is not None}

            # --- –¶–µ–Ω–∞ ---
            price_info = node.get('price', {}).get('amount', {})
            price_units = price_info.get('units')
            currency_code = price_info.get('currencyCode')

            # --- –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ ParsedAdItem ---
            item = ParsedAdItem()

            item['source_ad_id'] = node.get('id')
            item['url'] = node.get('url')
            item['source_name'] = "otomoto.pl"
            item['country_code'] = "PL"
            item['scraped_at'] = datetime.now(timezone.utc).isoformat()

            item['title'] = node.get('title')
            item['description'] = node.get('shortDescription')
            item['posted_on_source_at'] = node.get('createdAt')

            item['price'] = price_units
            item['currency'] = currency_code

            item['make_str'] = params.get('make')
            item['model_str'] = params.get('model')
            item['version_str'] = params.get('version')
            item['generation_str'] = params.get('generation')

            year_val = params.get('year')
            item['year'] = int(year_val) if year_val and str(year_val).isdigit() else None

            mileage_val = params.get('mileage')
            item['mileage'] = int(mileage_val) if mileage_val and str(mileage_val).isdigit() else None

            item['fuel_type_str'] = params.get('fuel_type')

            engine_capacity_val = params.get('engine_capacity')
            item['engine_capacity_cm3'] = int(engine_capacity_val) if engine_capacity_val and str(
                engine_capacity_val).isdigit() else None

            engine_power_val = params.get('engine_power')
            item['engine_power_hp'] = int(engine_power_val) if engine_power_val and str(
                engine_power_val).isdigit() else None

            item['gearbox_str'] = params.get('gearbox')
            item['transmission_str'] = params.get('transmission')
            item['color_str'] = params.get('color')

            location_data = node.get('location', {})
            item['city_str'] = location_data.get('city', {}).get('name')
            item['region_str'] = location_data.get('region', {}).get('name')

            item['seller_link'] = (node.get('sellerLink') or {}).get('id')

            main_photo = node.get('mainPhoto', {})
            if main_photo and main_photo.get('url'):
                item['image_urls'] = [main_photo.get('url')]
            else:
                item['image_urls'] = []

            # –î–æ–±–∞–≤–ª—è–µ–º ID –≤ –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏ –≤ –æ–±—â–∏–π –Ω–∞–±–æ—Ä
            if item['source_ad_id']:
                self.current_make_active_ids.add(item['source_ad_id'])
                self.scraped_ids.add(item['source_ad_id'])

            yield item

        # –û—Ç–º–µ—á–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        yield from self._handle_page_completion()


    def _handle_page_completion(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        self.current_make_processed_pages += 1
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É Scrapy
        self._update_scrapy_stats()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–∞—Ä–∫–∏
        if self.current_make_task is not None:
            if self.current_make_total_pages > 0:
                progress_percentage = (self.current_make_processed_pages / self.current_make_total_pages) * 100
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
                if hasattr(self, 'crawler'):
                    stats = self.crawler.stats
                    current_items = len(self.current_make_active_ids)
                    
                    description = (f"[yellow]{self.current_make_name}[/yellow] - "
                                f"—Å—Ç—Ä. {self.current_make_processed_pages}/{self.current_make_total_pages} "
                                f"({progress_percentage:.1f}%) ‚Ä¢ "
                                f"[bold]{current_items}[/bold] –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                else:
                    description = (f"[yellow]{self.current_make_name}[/yellow] - "
                                f"—Å—Ç—Ä. {self.current_make_processed_pages}/{self.current_make_total_pages} "
                                f"({progress_percentage:.1f}%)")
                
                self.progress.update(
                    self.current_make_task,
                    completed=self.current_make_processed_pages,
                    description=description
                )
            else:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è —Å 0 —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                self.progress.update(
                    self.current_make_task,
                    completed=1,
                    total=1,
                    description=f"[yellow]{self.current_make_name}[/yellow] - –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π (0)"
                )

        if (self.current_make_processed_pages >= max(self.current_make_total_pages, 1) 
            and not self.make_completion_lock):
            self.make_completion_lock = True
            yield from self._handle_make_completion()


    def _handle_make_completion(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –º–∞—Ä–∫–∏"""
        if self.current_make_name:
            # –ó–∞–≤–µ—Ä—à–∞–µ–º –∑–∞–¥–∞—á—É —Ç–µ–∫—É—â–µ–π –º–∞—Ä–∫–∏
            if self.current_make_task is not None:
                self.progress.update(
                    self.current_make_task,
                    completed=max(self.current_make_total_pages, 1),
                    description=f"[green]‚úÖ {self.current_make_name}[/green] - {len(self.current_make_active_ids)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π"
                )
            
            self.logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω –ø–∞—Ä—Å–∏–Ω–≥ –º–∞—Ä–∫–∏ {self.current_make_name}: {len(self.current_make_active_ids)} ID")
        
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º ActiveIdsItem
            active_ids_item = ActiveIdsItem(
                source_name="otomoto.pl",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ –∂–µ –∏–º—è, —á—Ç–æ –∏ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                make_str=self.current_make_name,
                ad_ids=list(self.current_make_active_ids)
            )
            
            dummy_request = scrapy.Request(
                url='data:,',
                callback=self._send_active_ids_item,
                meta={'active_ids_item': active_ids_item},
                dont_filter=True
            )
            yield dummy_request
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –º–∞—Ä–∫–µ
            self.current_make_index += 1
            next_request = self._get_request_for_current_make()
            if next_request:
                yield next_request
            else:
                # –ó–∞–≤–µ—Ä—à–∞–µ–º –æ–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
                if self.main_task is not None:
                    self.progress.update(self.main_task, completed=len(self.makes_list))
                
                self._stop_progress_bar()
                self._log_final_statistics()


    def _stop_progress_bar(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä"""
        if self.progress_live:
            time.sleep(1)  # –î–∞–µ–º –≤—Ä–µ–º—è —É–≤–∏–¥–µ—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            self.progress_live.stop()
            self.console.print("\n[bold green]üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω![/bold green]")


    def _log_final_statistics(self):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if hasattr(self, 'crawler'):
            stats = self.crawler.stats
            
            # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            total_time = time.time() - self.stats_start_time
            pages_crawled = stats.get_value('response_received_count', 0)
            items_scraped = stats.get_value('item_scraped_count', 0)
            
            pages_per_min = (pages_crawled / total_time) * 60 if total_time > 0 else 0
            items_per_min = (items_scraped / total_time) * 60 if total_time > 0 else 0
            
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Å–∏–≤—É—é —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
            table = Table(title="üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞")
            table.add_column("–ü–∞—Ä–∞–º–µ—Ç—Ä", style="cyan", width=25)
            table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="magenta", width=20)
            table.add_column("–°–∫–æ—Ä–æ—Å—Ç—å", style="green", width=15)
            
            table.add_row("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –º–∞—Ä–æ–∫", str(self.current_make_index), f"{self.current_make_index/(total_time/60):.1f}/–º–∏–Ω")
            table.add_row("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü", str(pages_crawled), f"{pages_per_min:.0f}/–º–∏–Ω")
            table.add_row("–°–æ–±—Ä–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π", str(items_scraped), f"{items_per_min:.0f}/–º–∏–Ω")
            table.add_row("–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã", time.strftime('%H:%M:%S', time.gmtime(total_time)), "")
            table.add_row("", "", "")  # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
            table.add_row("–û—à–∏–±–∫–∏ 403", str(self.error_stats['forbidden_403']), "")
            table.add_row("–û—à–∏–±–∫–∏ GraphQL", str(self.error_stats['graphql_errors']), "")
            table.add_row("–ü–æ–≤—Ç–æ—Ä—ã GraphQL", str(self.error_stats['graphql_retries']), "")
            table.add_row("–û—à–∏–±–∫–∏ JSON", str(self.error_stats['json_decode_errors']), "")
            
            self.console.print()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
            self.console.print(table)
            self.console.print()
            
            # –¢–∞–∫–∂–µ –ª–æ–≥–∏—Ä—É–µ–º –≤ –æ–±—ã—á–Ω—ã–π –ª–æ–≥ (–¥–ª—è —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–æ–≤)
            self.logger.info("=== –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê ===")
            self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –º–∞—Ä–æ–∫: {self.current_make_index}")
            self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {pages_crawled} ({pages_per_min:.0f}/–º–∏–Ω)")
            self.logger.info(f"–°–æ–±—Ä–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {items_scraped} ({items_per_min:.0f}/–º–∏–Ω)")
            self.logger.info(f"–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {time.strftime('%H:%M:%S', time.gmtime(total_time))}")
            self.logger.info(f"–û—à–∏–±–∫–∏: 403={self.error_stats['forbidden_403']}, GraphQL={self.error_stats['graphql_errors']}")


    def _send_active_ids_item(self, response):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç ActiveIdsItem —á–µ—Ä–µ–∑ pipeline"""
        active_ids_item = response.meta.get('active_ids_item')
        if active_ids_item:
            yield active_ids_item


    def _handle_403_error(self, response, context="unknown"):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç 403 –æ—à–∏–±–∫—É —Å –ª–æ–≥–∏–∫–æ–π –ø–∞—É–∑—ã"""
        self.error_stats['forbidden_403'] += 1
        self.consecutive_403_count += 1

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
        self._update_scrapy_stats()
        
        if self.current_make_task is not None:
            self.progress.update(
                self.current_make_task,
                description=f"[red]‚ö†Ô∏è {self.current_make_name}[/red] - 403 –æ—à–∏–±–∫–∞ ({self.consecutive_403_count}/{self.max_consecutive_403})"
            )
        
        self.logger.error(f"–ü–æ–ª—É—á–µ–Ω —Å—Ç–∞—Ç—É—Å 403 (Forbidden) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: {context}")
        self.logger.error(f"URL: {response.url}")
        self.logger.error(f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö 403 –æ—à–∏–±–æ–∫: {self.consecutive_403_count}/{self.max_consecutive_403}")
        
        if self.consecutive_403_count >= self.max_consecutive_403:
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–∞—É–∑–µ
            if self.current_make_task is not None:
                self.progress.update(
                    self.current_make_task,
                    description=f"[red]‚è∏Ô∏è {self.current_make_name}[/red] - –ø–∞—É–∑–∞ {self.pause_duration//60} –º–∏–Ω"
                )
            
            self.logger.warning(f"üö® –î–û–°–¢–ò–ì–ù–£–¢–û –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –ö–û–õ–ò–ß–ï–°–¢–í–û 403 –û–®–ò–ë–û–ö ({self.max_consecutive_403})")
            self.logger.warning(f"‚è∏Ô∏è  –°–¢–ê–í–ò–ú –ü–ê–†–°–ï–† –ù–ê –ü–ê–£–ó–£ –ù–ê {self.pause_duration//60} –ú–ò–ù–£–¢")
            self.is_paused = True
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
            resume_request = scrapy.Request(
                url='data:,',  # Dummy URL
                callback=self._resume_after_pause,
                dont_filter=True,
                meta={'download_delay': self.pause_duration}
            )
            return resume_request
        else:
            remaining = self.max_consecutive_403 - self.consecutive_403_count
            self.logger.info(f"‚ö†Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É. –î–æ –ø–∞—É–∑—ã –æ—Å—Ç–∞–ª–æ—Å—å {remaining} –æ—à–∏–±–æ–∫ 403")
            return None
    
    def _resume_after_pause(self, response):
        """–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ—Ç —Ä–∞–±–æ—Ç—É –ø–æ—Å–ª–µ –ø–∞—É–∑—ã"""
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –æ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏
        if self.current_make_task is not None:
            self.progress.update(
                self.current_make_task,
                description=f"[green]‚ñ∂Ô∏è {self.current_make_name}[/green] - –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"
            )
        
        self.logger.info(f"‚èØÔ∏è –í–û–ó–û–ë–ù–û–í–õ–Ø–ï–ú –†–ê–ë–û–¢–£ –ü–û–°–õ–ï –ü–ê–£–ó–´")
        self.logger.info(f"–°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö 403 –æ—à–∏–±–æ–∫")
        
        self.is_paused = False
        self.consecutive_403_count = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
        
        # –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å —Ç–µ–∫—É—â–µ–π –º–∞—Ä–∫–∏
        next_request = self._get_request_for_current_make()
        if next_request:
            yield next_request
        else:
            self.logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")
            self._log_final_statistics()
    
    def _handle_graphql_error(self, response, errors, context="unknown"):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç GraphQL –æ—à–∏–±–∫–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        self.error_stats['graphql_errors'] += 1

        self._update_scrapy_stats()
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –∏–∑ –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã—Ö
        retry_count = response.meta.get('graphql_retry_count', 0)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ Internal Error
        internal_errors = [e for e in errors if e.get('message') == 'Internal Error']
        
        if internal_errors and retry_count < self.graphql_max_retries:
            self.error_stats['graphql_retries'] += 1
            self.logger.warning(f"GraphQL Internal Error –≤ {context}. –ü–æ–ø—ã—Ç–∫–∞ {retry_count + 1}/{self.graphql_max_retries}")
            self.logger.info(f"–ü–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ {self.graphql_retry_delay} —Å–µ–∫—É–Ω–¥...")
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —Å—á–µ—Ç—á–∏–∫–æ–º –ø–æ–ø—ã—Ç–æ–∫
            new_meta = response.meta.copy()
            new_meta['graphql_retry_count'] = retry_count + 1
            new_meta['download_delay'] = self.graphql_retry_delay
            
            retry_request = response.request.replace(
                meta=new_meta,
                dont_filter=True
            )
            
            return retry_request
        else:
            if internal_errors:
                self.logger.error(f"GraphQL Internal Error –≤ {context} –ø–æ—Å–ª–µ {retry_count} –ø–æ–ø—ã—Ç–æ–∫. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            else:
                self.logger.error(f"GraphQL –æ—à–∏–±–∫–∏ –≤ {context}: {errors}")
            
            return None
    
    def _reset_403_counter_on_success(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—á–µ—Ç—á–∏–∫ 403 –æ—à–∏–±–æ–∫ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ"""
        if self.consecutive_403_count > 0:
            self.logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å. –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ 403 –æ—à–∏–±–æ–∫ (–±—ã–ª–æ: {self.consecutive_403_count})")
            self.consecutive_403_count = 0

    
    def spider_closed(self, spider):
        self._stop_progress_bar()

