#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import math
import scrapy
import urllib.parse as up
from collections import OrderedDict
from datetime import datetime, timezone
from ..items import ParsedAdItem, ActiveIdsItem
from ..utils.make_loader import MakeLoader
import os


class OtomotoSpider(scrapy.Spider):
    name = "otomoto"
    allowed_domains = ["otomoto.pl"]

    BASE_URL = "https://www.otomoto.pl/graphql"
    OPERATION_NAME = "listingScreen"
    ITEMS_PER_PAGE = 50

    EXTENSIONS = OrderedDict([
        ("persistedQuery", OrderedDict([
            ("sha256Hash", "1a840f0ab7fbe2543d0d6921f6c963de8341e04a4548fd1733b4a771392f900a"),
            ("version", 1),
        ]))
    ])

    BASE_FILTERS = [
        {"name": "category_id", "value": "29"},
        {"name": "new_used", "value": "used"},
    ]

    BASE_PARAMS = [
        "make", "vin", "offer_type", "show_pir", "fuel_type", "gearbox",
        "country_origin", "mileage", "engine_capacity", "color", "engine_code", 
        "transmission", "engine_power", "first_registration_year",
        "model", "version", "year", "generation"
    ]

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        """–°–æ–∑–¥–∞–µ—Ç spider —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º crawler'–∞"""
        spider = cls(*args, **kwargs)
        spider._set_crawler(crawler)
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ settings –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è spider
        spider.max_consecutive_403 = crawler.settings.getint('CONSECUTIVE_403_LIMIT', 3)
        spider.pause_duration = crawler.settings.getint('PAUSE_DURATION', 300)
        spider.graphql_retry_delay = crawler.settings.getint('GRAPHQL_RETRY_DELAY', 5)
        spider.graphql_max_retries = crawler.settings.getint('GRAPHQL_MAX_RETRIES', 3)
        
        spider.logger.info(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏ 403: –ª–∏–º–∏—Ç={spider.max_consecutive_403}, –ø–∞—É–∑–∞={spider.pause_duration}—Å")
        spider.logger.info(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏ GraphQL: –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {spider.graphql_retry_delay}—Å, –º–∞–∫—Å –ø–æ–≤—Ç–æ—Ä–æ–≤={spider.graphql_max_retries}")
        
        return spider

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.scraped_ids = set()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–∞—Ä–æ–∫
        make_loader = MakeLoader(self.logger)
        self.makes_list = make_loader.get_makes()
        self.current_make_index = 0
        self.current_make_name = None
        self.current_make_active_ids = set()

        self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.makes_list)} –º–∞—Ä–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
        
        # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.current_make_total_pages = 0
        self.current_make_processed_pages = 0
        self.make_completion_lock = False
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
        self.error_stats = {
            'forbidden_403': 0,
            'json_decode_errors': 0,
            'graphql_errors': 0,
            'graphql_retries': 0,
            'missing_data_errors': 0
        }
        
        # –°—á–µ—Ç—á–∏–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö 403 –æ—à–∏–±–æ–∫ (–∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        self.consecutive_403_count = 0
        self.max_consecutive_403 = 3  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.pause_duration = 300  # 5 –º–∏–Ω—É—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.is_paused = False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è GraphQL –æ—à–∏–±–æ–∫ (–∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        self.graphql_retry_delay = 5  # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
        self.graphql_max_retries = 3  # –ú–∞–∫—Å–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–æ–≤
        
        self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.makes_list)} –º–∞—Ä–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")


    def start_requests(self):
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ Scrapy –¥–ª—è –Ω–∞—á–∞–ª–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if not self.makes_list:
            self.logger.error("–°–ø–∏—Å–æ–∫ –º–∞—Ä–æ–∫ –ø—É—Å—Ç, –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω")
            return
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—É—é –º–∞—Ä–∫—É
        request = self._get_request_for_current_make()
        if request:
            yield request

    def _get_request_for_current_make(self):
        """–°–æ–∑–¥–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—É—â–µ–π –º–∞—Ä–∫–∏"""
        if self.current_make_index >= len(self.makes_list):
            self.logger.info("–í—Å–µ –º–∞—Ä–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            return None
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –º–∞—Ä–∫—É
        self.current_make_name = self.makes_list[self.current_make_index]
        self.current_make_active_ids = set()
        self.current_make_total_pages = 0
        self.current_make_processed_pages = 0
        self.make_completion_lock = False
        
        self.logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –º–∞—Ä–∫–∏: {self.current_make_name} ({self.current_make_index + 1}/{len(self.makes_list)})")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–∞—Ä–∫–∏
        self._update_filters_for_make(self.current_make_name)
        
        return scrapy.Request(
            url=self.build_url(page=1),
            callback=self.parse_initial,
            meta={'handle_httpstatus_list': [403], 'make_name': self.current_make_name}
        )

    def _update_filters_for_make(self, make_name):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–∞—Ä–∫–∏"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–∏–ª—å—Ç—Ä –º–∞—Ä–∫–∏
        self.BASE_FILTERS = [f for f in self.BASE_FILTERS if f.get('name') != 'filter_enum_make']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ñ–∏–ª—å—Ç—Ä –º–∞—Ä–∫–∏
        self.BASE_FILTERS.append({"name": "filter_enum_make", "value": make_name})

    def build_url(self, page: int) -> str:
        # ...existing code... (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å)
        variables = OrderedDict([
            ("filters", self.BASE_FILTERS),
            ("includeCepik", False),
            ("includeFiltersCounters", True),
            ("includeNewPromotedAds", False),
            ("includePriceEvaluation", False),
            ("includePromotedAds", False),
            ("includeRatings", False),
            ("includeSortOptions", False),
            ("includeSuggestedFilters", False),
            ("itemsPerPage", self.ITEMS_PER_PAGE),
            ("maxAge", 60),
            ("page", page),
            ("parameters", self.BASE_PARAMS),
            ("promotedInput", {})
        ])

        vars_compact = json.dumps(variables, separators=(',', ':'))
        ext_compact = json.dumps(self.EXTENSIONS, separators=(',', ':'))

        encoded_vars = up.quote(vars_compact)
        encoded_ext = up.quote(ext_compact)

        url = f"{self.BASE_URL}?operationName={self.OPERATION_NAME}&variables={encoded_vars}&extensions={encoded_ext}"
        return url

    def parse_initial(self, response):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–µ—Ä–≤—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –º–∞—Ä–∫–∏, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞ –ø–∞—É–∑–µ –ª–∏ –º—ã
        if self.is_paused:
            self.logger.info("–ü–∞—Ä—Å–µ—Ä –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –ø–∞—É–∑–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å")
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
        if response.status == 403:
            request = self._handle_403_error(response, context=f"parse_initial –¥–ª—è –º–∞—Ä–∫–∏ {self.current_make_name}")
            if request:
                yield request
                return
            else:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–∫—É—â—É—é –º–∞—Ä–∫—É –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π
                yield from self._handle_make_completion()
                return
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ 403 –æ—à–∏–±–æ–∫
        self._reset_403_counter_on_success()
        
        # ...existing code –¥–ª—è parse_initial...
        try:
            data = json.loads(response.text)
        except json.JSONDecodeError:
            self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å JSON —Å {response.url}")
            self.error_stats['json_decode_errors'] += 1
            yield from self._handle_make_completion()
            return

        if 'errors' in data:
            retry_request = self._handle_graphql_error(response, data['errors'], f"parse_initial –¥–ª—è –º–∞—Ä–∫–∏ {self.current_make_name}")
            if retry_request:
                yield retry_request
                return
            else:
                yield from self._handle_make_completion()
                return

        advert_search_data = data.get('data', {}).get('advertSearch')
        if not advert_search_data:
            self.logger.error(f"–ö–ª—é—á 'advertSearch' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            self.error_stats['missing_data_errors'] += 1
            yield from self._handle_make_completion()
            return

        total_ads = advert_search_data.get('totalCount', 0)
        self.current_make_total_pages = math.ceil(total_ads / self.ITEMS_PER_PAGE)
        self.logger.info(f"–ú–∞—Ä–∫–∞ {self.current_make_name}: –Ω–∞–π–¥–µ–Ω–æ {total_ads} –æ–±—ä—è–≤–ª–µ–Ω–∏–π, —Å—Ç—Ä–∞–Ω–∏—Ü: {self.current_make_total_pages}")

        # –ü–∞—Ä—Å–∏–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        yield from self.parse_page(response, response.meta)

        # –ó–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        for page_num in range(2, self.current_make_total_pages + 1):
            yield scrapy.Request(
                url=self.build_url(page=page_num),
                callback=self.parse_page,
                meta={'page_num': page_num, 'handle_httpstatus_list': [403], 'make_name': self.current_make_name}
            )

    def parse_page(self, response, meta=None):
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏"""
        current_meta = meta if meta else response.meta
        page_num = current_meta.get('page_num', 1)
        make_name = current_meta.get('make_name', self.current_make_name)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞ –ø–∞—É–∑–µ –ª–∏ –º—ã
        if self.is_paused:
            self.logger.info("–ü–∞—Ä—Å–µ—Ä –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –ø–∞—É–∑–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å")
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
        if response.status == 403:
            pause_request = self._handle_403_error(response, context=f"parse_page –º–∞—Ä–∫–∏ {make_name}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
            if pause_request:
                yield pause_request
                return
            else:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                yield from self._handle_page_completion()
                return
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–µ–Ω, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ 403 –æ—à–∏–±–æ–∫
        self._reset_403_counter_on_success()

        try:
            data = json.loads(response.text)
        except json.JSONDecodeError:
            self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å JSON –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num} —Å {response.url}")
            self.error_stats['json_decode_errors'] += 1
            yield from self._handle_page_completion()
            return

        if 'errors' in data:
            retry_request = self._handle_graphql_error(response, data['errors'], f"parse_page –º–∞—Ä–∫–∏ {make_name}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
            if retry_request:
                yield retry_request
                return
            else:
                yield from self._handle_page_completion()
                return

        advert_search_data = data.get('data', {}).get('advertSearch')
        if not advert_search_data:
            self.logger.error(f"–ö–ª—é—á 'advertSearch' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ JSON –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {response.text[:500]}")
            self.error_stats['missing_data_errors'] += 1
            yield from self._handle_page_completion()
            return

        edges = advert_search_data.get('edges', [])
        self.logger.info(f"–ú–∞—Ä–∫–∞ {make_name}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –Ω–∞–π–¥–µ–Ω–æ {len(edges)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")

        for edge in edges:
            node = edge.get('node', {})
            if not node:
                continue

            # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ---
            raw_params = node.get('parameters', [])
            params = {p.get('key'): p.get('value') for p in raw_params if p.get('key') and p.get('value') is not None}

            # --- –¶–µ–Ω–∞ ---
            price_info = node.get('price', {}).get('amount', {})
            price_units = price_info.get('units')
            currency_code = price_info.get('currencyCode')

            # --- –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ ParsedAdItem ---
            item = ParsedAdItem()

            item['source_ad_id'] = node.get('id')
            item['url'] = node.get('url')
            item['source_name'] = "otomoto.pl"
            item['country_code'] = "PL"
            item['scraped_at'] = datetime.now(timezone.utc).isoformat()

            item['title'] = node.get('title')
            item['description'] = node.get('shortDescription')
            item['posted_on_source_at'] = node.get('createdAt')

            item['price'] = price_units
            item['currency'] = currency_code

            item['make_str'] = params.get('make')
            item['model_str'] = params.get('model')
            item['version_str'] = params.get('version')
            item['generation_str'] = params.get('generation')

            year_val = params.get('year')
            item['year'] = int(year_val) if year_val and str(year_val).isdigit() else None

            mileage_val = params.get('mileage')
            item['mileage'] = int(mileage_val) if mileage_val and str(mileage_val).isdigit() else None

            item['fuel_type_str'] = params.get('fuel_type')

            engine_capacity_val = params.get('engine_capacity')
            item['engine_capacity_cm3'] = int(engine_capacity_val) if engine_capacity_val and str(
                engine_capacity_val).isdigit() else None

            engine_power_val = params.get('engine_power')
            item['engine_power_hp'] = int(engine_power_val) if engine_power_val and str(
                engine_power_val).isdigit() else None

            item['gearbox_str'] = params.get('gearbox')
            item['transmission_str'] = params.get('transmission')
            item['color_str'] = params.get('color')

            location_data = node.get('location', {})
            item['city_str'] = location_data.get('city', {}).get('name')
            item['region_str'] = location_data.get('region', {}).get('name')

            item['seller_link'] = (node.get('sellerLink') or {}).get('id')

            main_photo = node.get('mainPhoto', {})
            if main_photo and main_photo.get('url'):
                item['image_urls'] = [main_photo.get('url')]
            else:
                item['image_urls'] = []

            # –î–æ–±–∞–≤–ª—è–µ–º ID –≤ –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏ –≤ –æ–±—â–∏–π –Ω–∞–±–æ—Ä
            if item['source_ad_id']:
                self.current_make_active_ids.add(item['source_ad_id'])
                self.scraped_ids.add(item['source_ad_id'])

            yield item

        # –û—Ç–º–µ—á–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        yield from self._handle_page_completion()

    def _handle_page_completion(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        self.current_make_processed_pages += 1
        
        if (self.current_make_processed_pages >= self.current_make_total_pages 
            and not self.make_completion_lock):
            self.make_completion_lock = True
            yield from self._handle_make_completion()

    def _handle_make_completion(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –º–∞—Ä–∫–∏"""
        if self.current_make_name:
            self.logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω –ø–∞—Ä—Å–∏–Ω–≥ –º–∞—Ä–∫–∏ {self.current_make_name}: {len(self.current_make_active_ids)} ID")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º ActiveIdsItem
            active_ids_item = ActiveIdsItem(
                source_name="otomoto.pl",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ –∂–µ –∏–º—è, —á—Ç–æ –∏ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                make_str=self.current_make_name,
                ad_ids=list(self.current_make_active_ids)
            )
            
            dummy_request = scrapy.Request(
                url='data:,',
                callback=self._send_active_ids_item,
                meta={'active_ids_item': active_ids_item},
                dont_filter=True
            )
            yield dummy_request
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –º–∞—Ä–∫–µ
            self.current_make_index += 1
            next_request = self._get_request_for_current_make()
            if next_request:
                yield next_request
            else:
                # –ï—Å–ª–∏ –±–æ–ª—å—à–µ –Ω–µ—Ç –º–∞—Ä–æ–∫, –ª–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self._log_final_statistics()

    def _log_final_statistics(self):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        self.logger.info("=== –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê ===")
        self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –º–∞—Ä–æ–∫: {self.current_make_index}")
        self.logger.info(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(self.scraped_ids)}")
        self.logger.info(f"–û—à–∏–±–∫–∏ 403 (Forbidden): {self.error_stats['forbidden_403']}")
        self.logger.info(f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö 403 –≤ –∫–æ–Ω—Ü–µ: {self.consecutive_403_count}")
        self.logger.info(f"–°—Ç–∞—Ç—É—Å –ø–∞—É–∑—ã: {'–î–ê' if self.is_paused else '–ù–ï–¢'}")
        self.logger.info(f"–û—à–∏–±–∫–∏ GraphQL: {self.error_stats['graphql_errors']}")
        self.logger.info(f"–ü–æ–≤—Ç–æ—Ä—ã GraphQL: {self.error_stats['graphql_retries']}")
        self.logger.info(f"–û—à–∏–±–∫–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON: {self.error_stats['json_decode_errors']}")
        self.logger.info(f"–û—à–∏–±–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –¥–∞–Ω–Ω—ã—Ö: {self.error_stats['missing_data_errors']}")
        self.logger.info("=====================================")
        
        if self.consecutive_403_count > 0:
            self.logger.warning(f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å {self.consecutive_403_count} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ 403 –æ—à–∏–±–∫–∞–º–∏")
            self.logger.warning("–í–æ–∑–º–æ–∂–Ω–æ, —Å–µ—Ä–≤–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–∏–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫—É")
        
        if self.error_stats['graphql_errors'] > 0:
            success_rate = ((self.error_stats['graphql_errors'] - self.error_stats['graphql_retries']) / self.error_stats['graphql_errors']) * 100
            self.logger.info(f"GraphQL —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–æ–≤: {success_rate:.1f}%")

    def _send_active_ids_item(self, response):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç ActiveIdsItem —á–µ—Ä–µ–∑ pipeline"""
        active_ids_item = response.meta.get('active_ids_item')
        if active_ids_item:
            yield active_ids_item

    def _handle_403_error(self, response, context="unknown"):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç 403 –æ—à–∏–±–∫—É —Å –ª–æ–≥–∏–∫–æ–π –ø–∞—É–∑—ã"""
        self.error_stats['forbidden_403'] += 1
        self.consecutive_403_count += 1
        
        self.logger.error(f"–ü–æ–ª—É—á–µ–Ω —Å—Ç–∞—Ç—É—Å 403 (Forbidden) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: {context}")
        self.logger.error(f"URL: {response.url}")
        self.logger.error(f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö 403 –æ—à–∏–±–æ–∫: {self.consecutive_403_count}/{self.max_consecutive_403}")
        
        if self.consecutive_403_count >= self.max_consecutive_403:
            self.logger.warning(f"üö® –î–û–°–¢–ò–ì–ù–£–¢–û –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –ö–û–õ–ò–ß–ï–°–¢–í–û 403 –û–®–ò–ë–û–ö ({self.max_consecutive_403})")
            self.logger.warning(f"‚è∏Ô∏è  –°–¢–ê–í–ò–ú –ü–ê–†–°–ï–† –ù–ê –ü–ê–£–ó–£ –ù–ê {self.pause_duration//60} –ú–ò–ù–£–¢")
            self.is_paused = True
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
            resume_request = scrapy.Request(
                url='data:,',  # Dummy URL
                callback=self._resume_after_pause,
                dont_filter=True,
                meta={'download_delay': self.pause_duration}
            )
            return resume_request
        else:
            remaining = self.max_consecutive_403 - self.consecutive_403_count
            self.logger.info(f"‚ö†Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É. –î–æ –ø–∞—É–∑—ã –æ—Å—Ç–∞–ª–æ—Å—å {remaining} –æ—à–∏–±–æ–∫ 403")
            return None
    
    def _resume_after_pause(self, response):
        """–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ—Ç —Ä–∞–±–æ—Ç—É –ø–æ—Å–ª–µ –ø–∞—É–∑—ã"""
        self.logger.info(f"‚èØÔ∏è  –í–û–ó–û–ë–ù–û–í–õ–Ø–ï–ú –†–ê–ë–û–¢–£ –ü–û–°–õ–ï –ü–ê–£–ó–´")
        self.logger.info(f"–°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö 403 –æ—à–∏–±–æ–∫")
        
        self.is_paused = False
        self.consecutive_403_count = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
        
        # –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å —Ç–µ–∫—É—â–µ–π –º–∞—Ä–∫–∏
        next_request = self._get_request_for_current_make()
        if next_request:
            yield next_request
        else:
            self.logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")
            self._log_final_statistics()
    
    def _handle_graphql_error(self, response, errors, context="unknown"):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç GraphQL –æ—à–∏–±–∫–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        self.error_stats['graphql_errors'] += 1
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –∏–∑ –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã—Ö
        retry_count = response.meta.get('graphql_retry_count', 0)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ Internal Error
        internal_errors = [e for e in errors if e.get('message') == 'Internal Error']
        
        if internal_errors and retry_count < self.graphql_max_retries:
            self.error_stats['graphql_retries'] += 1
            self.logger.warning(f"GraphQL Internal Error –≤ {context}. –ü–æ–ø—ã—Ç–∫–∞ {retry_count + 1}/{self.graphql_max_retries}")
            self.logger.info(f"–ü–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ {self.graphql_retry_delay} —Å–µ–∫—É–Ω–¥...")
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —Å—á–µ—Ç—á–∏–∫–æ–º –ø–æ–ø—ã—Ç–æ–∫
            new_meta = response.meta.copy()
            new_meta['graphql_retry_count'] = retry_count + 1
            new_meta['download_delay'] = self.graphql_retry_delay
            
            retry_request = response.request.replace(
                meta=new_meta,
                dont_filter=True
            )
            
            return retry_request
        else:
            if internal_errors:
                self.logger.error(f"GraphQL Internal Error –≤ {context} –ø–æ—Å–ª–µ {retry_count} –ø–æ–ø—ã—Ç–æ–∫. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            else:
                self.logger.error(f"GraphQL –æ—à–∏–±–∫–∏ –≤ {context}: {errors}")
            
            return None
    
    def _reset_403_counter_on_success(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—á–µ—Ç—á–∏–∫ 403 –æ—à–∏–±–æ–∫ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ"""
        if self.consecutive_403_count > 0:
            self.logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å. –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ 403 –æ—à–∏–±–æ–∫ (–±—ã–ª–æ: {self.consecutive_403_count})")
            self.consecutive_403_count = 0

